{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "- Stochastic Gradient Descent (SGD) updates the parameters of the model using only a single sample or a small batch of samples, which is a stochastic approximation of the gradient descent optimization.\n",
    "\n",
    "- The term \"stochastic\" comes from the fact that **the gradient based on a single sample is a \"stochastic\" approximation of the true gradient based on the entire dataset**.\n",
    "\n",
    "# Process of SGD\n",
    "1. **Initialization**: Start with an initial set of parameters.\n",
    "2. **Random Sample**: Randomly shuffle the dataset and pick one sample (or a small batch).\n",
    "3. **Compute Gradient**: Calculate the gradient of the loss function with respect to the parameters for that single sample/batch.\n",
    "4. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient by a factor called the learning rate.\n",
    "5. **Repeat**: Repeat steps 2-4 until the algorithm converges, which means the parameters have stabilized and do not change significantly with further iterations.\n",
    "\n",
    "\n",
    "# Effects of Using SGD:\n",
    "\n",
    "- Convergence Speed: SGD can converge faster because it updates the parameters more frequently.\n",
    "- Memory Efficiency: It's more memory-efficient, particularly with large datasets, because it doesn't require computing gradients for the whole dataset at once.\n",
    "- Escape Local Minima: It has a better chance of escaping local minima in the loss landscape due to the noisy nature of the updates.\n",
    "\n",
    "# How to?\n",
    "- In scikit-learn, the **SGDRegressor** and **SGDClassifier** are the two main classes that implement Stochastic Gradient Descent for regression and classification tasks, respectively.\n",
    "\n",
    "## Parameters Instruction:\n",
    "\n",
    "- loss: This is the loss function to be used. The default value 'squared_loss' refers to the ordinary least squares fit. Other options include 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive' for different scenarios and robustness to outliers.\n",
    "\n",
    "- penalty: The penalty (aka regularization term) to be used. 'l2' is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might be used when you want to have a sparse model, where few features contribute to the prediction with non-zero coefficients.\n",
    "\n",
    "- alpha: This is the regularization term that combats overfitting by constraining the size of the weights. **Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, but too high a value will lead to underfitting**.\n",
    "\n",
    "- max_iter and tol: These two parameters control the termination condition of the algorithm. max_iter sets the maximum number of epochs (passes over the entire dataset), while tol specifies the stopping criterion related to the improvement of the loss function.\n",
    "\n",
    "- learning_rate: The strategy for updating the learning rate during training. 'constant' keeps the learning rate constant, as given by 'eta0'. 'optimal' is the default and adjusts the learning rate automatically based on the number of samples (recommended for large datasets).\n",
    "\n",
    "- eta0: The initial learning rate when the learning rate schedule is 'constant', 'invscaling', or 'adaptive'.\n",
    "\n",
    "- early_stopping, validation_fraction, n_iter_no_change: These parameters control the early stopping feature. If early_stopping is set to True, then a fraction of the training data as specified by validation_fraction is set aside to evaluate the stopping criterion, which is then assessed every n_iter_no_change number of epochs.\n",
    "\n",
    "- random_state: The seed for the pseudo-random number generator used when shuffling the data for stochastic optimization. This is important for reproducibility of results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "regressor = SGDRegressor(\n",
    "    loss='squared_loss',  # Default loss function for regression; equivalent to ordinary least squares.\n",
    "    penalty='l2',  # Default penalty is L2 regularization.\n",
    "    alpha=0.0001,  # Constant that multiplies the regularization term.\n",
    "    max_iter=1000,  # Maximum number of passes over the training data.\n",
    "    tol=1e-3,  # The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).\n",
    "    learning_rate='invscaling',  # The learning rate schedule.\n",
    "    eta0=0.01,  # The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules.\n",
    "    power_t=0.25,  # The exponent for inverse scaling learning rate.\n",
    "    early_stopping=False,  # Whether to use early stopping to terminate training when validation score is not improving.\n",
    "    validation_fraction=0.1,  # The proportion of training data to set aside as validation set for early stopping.\n",
    "    n_iter_no_change=5,  # Number of iterations with no improvement to wait before early stopping.\n",
    "    random_state=None  # The seed of the pseudo random number generator to use when shuffling the data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Examples\n",
    "\n",
    "## NOTE: Feature Scaling is important for SGD\n",
    "- SGD relies on the gradient of the loss function to update the parameters. The gradient measures how much the loss would change if you changed the parameters. **If one feature has a much larger range of values than another, it will dominate the gradient and can cause the learning process to be biased towards that particular feature**. This can make the optimization process inefficient, because **SGD might take much smaller steps in the direction of other features compared to the feature with the larger range of values**.\n",
    "\n",
    "\n",
    "    Example: Consider a dataset with two features where one feature is the size of a house in square feet (ranging from 1,000 to 10,000) and the other is the number of bedrooms (which usually ranges from 1 to 5). Without scaling, the 'size' feature would have much larger numeric values and would disproportionately influence the gradient.\n",
    "    - One common method is to use `StandardScaler` from scikit-learn to standardize features by removing the mean and scaling to unit variance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.9999997358874672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate some synthetic data for regression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "regressor = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = regressor.score(X_test, y_test)\n",
    "print(f\"Model Score: {score}\")\n",
    "\n",
    "# Predict new values\n",
    "y_pred = regressor.predict(X_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T00:03:08.223698Z",
     "start_time": "2024-01-23T00:03:07.576140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
