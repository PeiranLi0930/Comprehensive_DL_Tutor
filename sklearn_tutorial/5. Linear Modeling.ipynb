{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Linear Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression: Explore the (Linear) relationship between Y-X\n",
    "- Method: **sklearn.linear_model.LinearRegression**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 1.0\n",
      "Model Coefficients: [1. 2.]\n",
      "Model Intercept: 3.0000000000000018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "model = LinearRegression(fit_intercept = True, copy_X = True, n_jobs = 3).fit(X, y)\n",
    "\n",
    "prediction = model.predict(np.array([[3, 5]]))\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")# evaluate the performance of the model, 0~1, 1-best score.\n",
    "print(f\"Model Coefficients: {model.coef_}\")\n",
    "print(f\"Model Intercept: {model.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:01:16.617470Z",
     "start_time": "2023-10-29T20:01:16.613751Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression: Estimate 0/1\n",
    "\n",
    "- Method: sklearn.linear_model.LogisticRegression\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.96\n",
      "Actual iter number for each class: [7 7 6]\n",
      "Intercept/Bias: [ 0.26421853  1.09392467 -1.21470917]\n",
      "Total Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y = True) # (150, 4), (150,)\n",
    "\n",
    "model = LogisticRegression(penalty = \"l2\", tol = 1e-4, solver = \"liblinear\", multi_class = \"auto\").fit(X, y)\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")\n",
    "print(f\"Actual iter number for each class: {model.n_iter_}\")\n",
    "print(f\"Intercept/Bias: {model.intercept_}\")\n",
    "print(f\"Total Classes: {model.classes_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:42:33.844771Z",
     "start_time": "2023-10-29T20:42:33.838397Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Ridge Regression: Add L2 Regularization to Loss Function\n",
    "$$\\sum_{j=1}^m\\left(Y_i-W_0-\\sum_{i=1}^n W_i X_{j i}\\right)^2+\\alpha \\sum_{i=1}^n W_i^2=\\text { loss_function }+\\alpha \\sum_{i=1}^n W_i^2$$\n",
    "- Method: sklearn.linear_model.Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.930087491805933\n",
      "Model Coefficients: [-0.11347865 -0.03188039  0.25933952  0.53762684]\n",
      "Bias: 0.14117085854172984\n",
      "Iter: [28]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(solver = \"sag\").fit(X, y)\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")\n",
    "print(f\"Model Coefficients: {model.coef_}\")\n",
    "print(f\"Bias: {model.intercept_}\")\n",
    "print(f\"Iter: {model.n_iter_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T21:03:06.743429Z",
     "start_time": "2023-10-29T21:03:06.740428Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Bayesian Regression: Allows a natural mechanism to survive insufficient data or poorly distributed data by formulating Linear Regression using Probability Distributors rather than point estimators\n",
    "\n",
    "#### Bayes Theorem\n",
    "\n",
    "$$ P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)} $$\n",
    "- $P(A \\mid B)$ is the posterior probability: The probability of event $\\mathrm{A}$ occurring given that $B$ is true.\n",
    "- $P(B \\mid A)$ is the likelihood: The probability of observing $\\mathrm{B}$ given that $\\mathrm{A}$ is true.\n",
    "- $P(A)$ is the prior probability: The initial probability of event $\\mathrm{A}$.\n",
    "- $P(B)$ is the evidence: The overall probability of event $B$.\n",
    "\n",
    "#### Bayesian Inference\n",
    "Bayesian inference uses Bayes' theorem to **update the probability estimate for a hypothesis as more evidence or information becomes available**. \n",
    "- It's about belief revision: you start with a prior belief (prior probability), and as new data comes in, you update your belief to form a new, revised belief (posterior probability).\n",
    "\n",
    "**In this process, the main takeaway is that instead of just getting the \"best guess\" (as in classical regression), Bayesian Ridge Regression gives you a \"range of guesses\" along with how confident you are in each guess. This is particularly useful when making decisions in the face of uncertainty.**\n",
    "\n",
    "##### $P(data)$ evidence: the probability of observing the data under all possible values of parameters.\n",
    "\n",
    "- Essentially serves as a normalization factor to ensure that the posterior distribution is a true probability distribution that sums/integrates = 1\n",
    "- $P(\\text { data })=\\int P(\\text { data } \\mid \\beta) \\cdot P(\\beta) d \\beta$\n",
    "\n",
    "#### Bayesian & Traditional Linear Regression\n",
    "- Traditional linear regression gives you point estimates for the output $y$ given inputs $X$ and weights $w$.\n",
    "- Bayesian regression, on the other hand, treats the output $y$ as a random variable with a probability distribution. This allows the model to express uncertainty in its predictions.\n",
    "#### Bayesian Ridge Regression\n",
    "- In Bayesian Ridge regression, we start with a prior belief about the distribution of the weights $w$.\n",
    "- The prior is assumed to be a **spherical Gaussian**, which means **it's a multivariate normal distribution where all dimensions are independent and have the same variance**.\n",
    "- The notation $p(w \\mid \\lambda)$ denotes the probability of the weights given the hyperparameter $\\lambda$, which controls the spread of the prior distribution (a larger $\\lambda$ means a tighter spread around zero, acting as regularization).\n",
    "- The formula $w \\sim \\mathcal{N}\\left(0, \\lambda^{-1} I_p\\right)$ states that the prior distribution of the weights is centered at zero with a covariance matrix $\\lambda^{-1} I_p$ where $I_p$ is the identity matrix of size $p$, and $p$ is the number of features (or predictors)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [0.49999993 0.49999993]\n",
      "Intercept: 1.9999946720972162e-07\n",
      "Estimated Precision of the noise: 1500000.9999922747\n",
      "Estimated Precision of the weight: 1.9999962667112887\n",
      "Iterations: 4\n",
      "Estimated Covariance of the weights: [[ 0.2500005  -0.25000043]\n",
      " [-0.25000043  0.2500005 ]]\n",
      "Accuracy: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "x = [[0, 0], [1, 1], [2, 2], [3, 3]]\n",
    "y = [0, 1, 2, 3]\n",
    "\n",
    "bayes_model = BayesianRidge()\n",
    "bayes_model.fit(x, y)\n",
    "\n",
    "\n",
    "print(f\"Weight: {bayes_model.coef_}\")\n",
    "print(f\"Intercept: {bayes_model.intercept_}\")\n",
    "print(f\"Estimated Precision of the noise: {bayes_model.alpha_}\")\n",
    "print(f\"Estimated Precision of the weight: {bayes_model.lambda_}\")\n",
    "print(f\"Iterations: {bayes_model.n_iter_}\")\n",
    "print(f\"Estimated Covariance of the weights: {bayes_model.sigma_}\")\n",
    "print(f\"Accuracy: {bayes_model.scores_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T21:18:36.744884Z",
     "start_time": "2023-12-23T21:18:36.734122Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### LASSO: Add L1 Regularization to Loss Function\n",
    "$$\\sum_{j=1}^m\\left(Y_i-W_0-\\sum_{i=1}^n W_i X_{j i}\\right)^2+\\alpha \\sum_{i=1}^n\\left|W_i\\right|=\\text { loss_function }+\\alpha \\sum_{i=1}^n\\left|W_i\\right|$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: [0.03  1.015 2.    2.985 3.97 ]\n",
      "R-squared: 0.999775\n",
      "MSE: 0.00045000000000000216\n",
      "MAE: 0.018000000000000016\n",
      "Weights: [9.85000000e-01 2.77555756e-17]\n",
      "Intercept: 0.030000000000000027\n",
      "Iterations: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "x = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n",
    "y = [0, 1, 2, 3, 4]\n",
    "lasso = Lasso(alpha = 0.03) # the penalty intensity\n",
    "lasso.fit(x, y)\n",
    "print(f\"Predict: {lasso.predict(x)}\")\n",
    "print(f\"R-squared: {r2_score(y, lasso.predict(x))}\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(f\"MSE: {mean_squared_error(y, lasso.predict(x))}\")\n",
    "print(f\"MAE: {mean_absolute_error(y, lasso.predict(x))}\")\n",
    "\n",
    "print(f\"Weights: {lasso.coef_}\")\n",
    "print(f\"Intercept: {lasso.intercept_}\")\n",
    "print(f\"Iterations: {lasso.n_iter_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T21:39:34.494557Z",
     "start_time": "2023-12-23T21:39:34.490687Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Multi-task Lasso: trained with L1 + L2 mixed regularization, estimates sparse coefficients for multiple regression problem jointly.\n",
    "\n",
    "- Multi-task is useful when you believe that the tasks are related and can benefit from being learned together because the shared structure among the tasks can be exploited.\n",
    "\n",
    "#### Example\n",
    "Imagine you're managing a chain of stores, and you want to predict the daily sales of multiple product categories across your stores. You have features like store location, day of the week, marketing spend, and holidays. While each product category is a separate regression problem, they share common features and potentially some underlying sales patterns.\n",
    "\n",
    "Here, each \"task\" is the prediction of sales for a specific product category:\n",
    "\n",
    "Task 1: Predicting daily sales of electronics.\n",
    "Task 2: Predicting daily sales of clothing.\n",
    "Task 3: Predicting daily sales of groceries.\n",
    "Instead of building three separate models, one for each category, you build a multi-task model with shared feature sets but separate outputs for each category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electrinics, Clothing, Groceries: [[258.88582797 194.16437098  79.44291399]]\n",
      "Model Coefficients: [[1.94429140e+01 1.21842475e-14]\n",
      " [1.45821855e+01 9.13818561e-15]\n",
      " [9.72145699e+00 6.09212374e-15]]\n",
      "Model Intercept: [181.11417203 135.83562902  40.55708601]\n",
      "Model Iterations: 2\n",
      "Model Score: 0.9992241379310345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLasso\n",
    "import numpy as np\n",
    "\n",
    "# Feature matrix (each row corresponds to data for one day)\n",
    "X = np.array([\n",
    "    [1, 2],  # Features for Day 1\n",
    "    [2, 3],  # Features for Day 2\n",
    "    [3, 4],  # Features for Day 3\n",
    "    # ... and so on for more days\n",
    "])\n",
    "\n",
    "# Target matrix (each row corresponds to sales for each product category)\n",
    "Y = np.array([\n",
    "    [200, 150, 50],  # Sales for Day 1: [electronics, clothing, groceries]\n",
    "    [220, 165, 60],  # Sales for Day 2: [electronics, clothing, groceries]\n",
    "    [240, 180, 70],  # Sales for Day 3: [electronics, clothing, groceries]\n",
    "    # ... and so on for more days\n",
    "])\n",
    "\n",
    "# Initialize the MultiTaskLasso model\n",
    "multi_task_lasso = MultiTaskLasso(alpha=0.5)\n",
    "\n",
    "# Fit the model to the data\n",
    "multi_task_lasso.fit(X, Y)\n",
    "\n",
    "# Predict sales for a new day with given features\n",
    "new_X = np.array([[4, 5]])  # Features for the new day\n",
    "predicted_sales = multi_task_lasso.predict(new_X)\n",
    "\n",
    "print(f\"Electrinics, Clothing, Groceries: {predicted_sales}\")\n",
    "print(f\"Model Coefficients: {multi_task_lasso.coef_}\")\n",
    "print(f\"Model Intercept: {multi_task_lasso.intercept_}\")\n",
    "print(f\"Model Iterations: {multi_task_lasso.n_iter_}\")\n",
    "print(f\"Model Score: {multi_task_lasso.score(X, Y)}\")\n",
    "\n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T21:53:28.541376Z",
     "start_time": "2023-12-23T21:53:28.507529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Elastic-Net: linearly combines both L1 and L2 penalty. \n",
    "\n",
    "$$\\min _w \\frac{1}{2 n_{\\text {samples }}}\\left\\|X_w-y\\right\\|_2^2+\\alpha \\rho\\|w\\|_1+\\frac{\\alpha(1-\\rho)}{2}\\|w\\|_2^2$$\n",
    "\n",
    "- It's useful when there are multiple correlated features\n",
    "- Lasso: feature selection   Ridge: handle correlated variables. \n",
    "- useful when you have data with many features, some of which are correlated. \n",
    "- Elastic-Net can be a more stable choice over Lasso and Ridge individually, especially when dealing with datasets where multiple features have a relationship or when the number of predictors is more than the number of observations.\n",
    "\n",
    "#### Lasso & Elastic-Net\n",
    "- Lasso can result in **sparse models** where only a subset of all possible predictors is used. \n",
    "     - This is particularly useful for **feature selection** when you have a large number of features.\n",
    "- As for **Dealing With Correlated Features**: \n",
    "    - In the case of **highly correlated features**, Lasso tends to select one of them at random and shrinks the others to 0, which can be somewhat arbitrary and depends on the data and the other of the features. \n",
    "    - Elastic-Net, on the other hand, is likely to include both correlated features in the model but with **their coefficients shrunk towards each other and possibly towards zero**. This is because Elastic-Net includes the L2 penalty, which does not enforce sparsity as strongly as the L1 penalty.\n",
    "- Combination of L1 and L2 Regularization: \n",
    "    1. produce sparse model (L1)\n",
    "    2. The L2 part of the penalty helps to handle correlated features more effectively than Lasso alone. It tends to **shrink correlated predictors towards each other**, which means that if one feature in a group of correlated features has a non-zero coefficient, the others are likely to as well. This \"grouping effect\" is beneficial when predictors are correlated.\n",
    "       - include all features in the model but with the coefficients shrink to 0.\n",
    "       - can lead to better performance when the number of independent features (predictors) is large compared of observations(data points), or when several features \n",
    "\n",
    "#### Cyclic or Random ?\n",
    "- Cyclic updating is systematic and ensures that all features are considered in every cycle, which can be **more stable**. \n",
    "- Random updating can *potentially escape local minima* in the optimization landscape by introducing randomness, which **might lead to faster convergence in some cases, but it can also be less stable**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-24T03:08:49.809932Z",
     "start_time": "2023-12-24T03:08:49.641080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [57.77248544 13.23491871]\n",
      "Intercept: -0.5103725681526683\n",
      "Number of iterations: 3\n",
      "R-squared: 0.9980894184240998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_features=2, noise=0.1)\n",
    "\n",
    "# Create an ElasticNet regression model instance\n",
    "# Note: L1_ratio between 0 and 1 controls the balance between L1 and L2 regularization (0.5 is an equal balance)\n",
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Fit the model\n",
    "elastic_net_model.fit(X, y)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred = elastic_net_model.predict(X)\n",
    "\n",
    "# Print model properties\n",
    "print(f\"Coefficients: {elastic_net_model.coef_}\")\n",
    "print(f\"Intercept: {elastic_net_model.intercept_}\")\n",
    "print(f\"Number of iterations: {elastic_net_model.n_iter_}\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiTaskElasticNet\n",
    "Following is the objective function to minimize:\n",
    "$$\n",
    "\\min _w \\frac{1}{2 n_{\\text {samples }}}\\left\\|X_w-y\\right\\|_{\\text {Fro }}^2+\\alpha \\rho\\|w\\|_{21}+\\frac{\\alpha(1-\\rho)}{2}\\|w\\|_{\\text {Fro }}^2\n",
    "$$\n",
    "\n",
    "As in MultiTaskLasso, here also, Fro indicates the Frobenius norm:\n",
    "$$\n",
    "\\|A\\|_{F r o}=\\sqrt{\\sum_{i j} a_{i j}^2}\n",
    "$$\n",
    "\n",
    "And L1L2 leads to the following:\n",
    "$$\n",
    "\\|A\\|_{21}=\\sum_i \\sqrt{\\sum_j a_{i j}^2}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[63.36710479 69.43852685 16.98190953 33.48697788 79.2751291  64.9175761\n",
      "  61.82069432 98.76607078 98.26524367 74.99887439]\n",
      " [17.5410879  44.59688814  5.43184517 89.02715564  9.03561985 35.94878403\n",
      "  16.94801022 55.66961899 11.11050918 94.5492273 ]\n",
      " [57.88935148 90.78696675 40.73576259 44.32712342 40.74797597 12.58649269\n",
      "   9.41150463 63.18893214 51.3272421  12.73376737]]\n",
      "Intercepts: [ 0.15607329 -0.03357989  0.11068505]\n",
      "Score: 0.9999765070436003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "# Here, n_targets > 1 creates multiple y values for each X, suitable for multi-task learning\n",
    "X, Y = make_regression(n_samples=100, n_features=10, n_targets=3, noise=0.1)\n",
    "\n",
    "# Create a MultiTaskElasticNet regression model instance\n",
    "# alpha: Constant that multiplies the penalty terms and thus determines the level of regularization\n",
    "# l1_ratio: The ElasticNet mixing parameter, with 0 < l1_ratio <= 1; l1_ratio=1 corresponds to Lasso; l1_ratio = 0 to Ridge.\n",
    "multi_task_elastic_net_model = MultiTaskElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "\n",
    "# Fit the model\n",
    "multi_task_elastic_net_model.fit(X, Y) \n",
    "\n",
    "# The coefficients\n",
    "print(f\"Coefficients: {multi_task_elastic_net_model.coef_}\")\n",
    "\n",
    "# The intercepts\n",
    "print(f\"Intercepts: {multi_task_elastic_net_model.intercept_}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "Y_pred = multi_task_elastic_net_model.predict(X)\n",
    "\n",
    "# The model has a `.score` method that returns the coefficient of determination R^2 of the prediction.\n",
    "# The score method estimates the performance of the model on the training set\n",
    "score = multi_task_elastic_net_model.score(X, Y)\n",
    "print(f\"Score: {score}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T03:15:36.720680Z",
     "start_time": "2023-12-24T03:15:36.698350Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
