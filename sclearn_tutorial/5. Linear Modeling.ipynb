{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Linear Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression: Explore the (Linear) relationship between Y-X\n",
    "- Method: sklearn.linear_model.LinearRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 1.0\n",
      "Model Coefficients: [1. 2.]\n",
      "Model Intercept: 3.0000000000000018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "model = LinearRegression(fit_intercept = True, copy_X = True, n_jobs = 3).fit(X, y)\n",
    "\n",
    "prediction = model.predict(np.array([[3, 5]]))\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")# evaluate the performance of the model, 0~1, 1-best score.\n",
    "print(f\"Model Coefficients: {model.coef_}\")\n",
    "print(f\"Model Intercept: {model.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:01:16.617470Z",
     "start_time": "2023-10-29T20:01:16.613751Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression: Estimate 0/1\n",
    "\n",
    "- Method: sklearn.linear_model.LogisticRegression\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.96\n",
      "Actual iter number for each class: [7 7 6]\n",
      "Intercept/Bias: [ 0.26421853  1.09392467 -1.21470917]\n",
      "Total Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y = True) # (150, 4), (150,)\n",
    "\n",
    "model = LogisticRegression(penalty = \"l2\", tol = 1e-4, solver = \"liblinear\", multi_class = \"auto\").fit(X, y)\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")\n",
    "print(f\"Actual iter number for each class: {model.n_iter_}\")\n",
    "print(f\"Intercept/Bias: {model.intercept_}\")\n",
    "print(f\"Total Classes: {model.classes_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:42:33.844771Z",
     "start_time": "2023-10-29T20:42:33.838397Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Ridge Regression: Add L2 Regularization to Loss Function\n",
    "$$\\sum_{j=1}^m\\left(Y_i-W_0-\\sum_{i=1}^n W_i X_{j i}\\right)^2+\\alpha \\sum_{i=1}^n W_i^2=\\text { loss_function }+\\alpha \\sum_{i=1}^n W_i^2$$\n",
    "- Method: sklearn.linear_model.Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.930087491805933\n",
      "Model Coefficients: [-0.11347865 -0.03188039  0.25933952  0.53762684]\n",
      "Bias: 0.14117085854172984\n",
      "Iter: [28]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(solver = \"sag\").fit(X, y)\n",
    "\n",
    "print(f\"Model Score: {model.score(X, y)}\")\n",
    "print(f\"Model Coefficients: {model.coef_}\")\n",
    "print(f\"Bias: {model.intercept_}\")\n",
    "print(f\"Iter: {model.n_iter_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T21:03:06.743429Z",
     "start_time": "2023-10-29T21:03:06.740428Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "##### Bayesian Regression: Allows a natural mechanism to survive insufficient data or poorly distributed data by formulating Linear Regression using Probability Distributors rather than point estimators\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### LASSO: Add L1 Regularization to Loss Function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Multi-task Lasso: trained with L1 + L2 mixed regularization, estimates sparse coefficients for multiple regression problem jointly.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Elastic-Net: linearly combines both L1 and L2 penalty. It's useful when there are multiple correlated features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
