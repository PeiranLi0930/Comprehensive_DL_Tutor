{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHBkAdENa5FY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB2cQUShkXNm"
      },
      "source": [
        "# Ungraded Lab: ImageDataGenerator with a Validation Set\n",
        "\n",
        "In this lab, you will continue using the `ImageDataGenerator` class to prepare the `Horses or Humans` dataset. This time, you will add a validation set so you can also measure how well the model performs on data it hasn't seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsO-u_3fySMd"
      },
      "source": [
        "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5FfBGV5yUjb"
      },
      "source": [
        "Run the code blocks below to download the datasets `horse-or-human.zip` and `validation-horse-or-human.zip` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZT2UsyIVe_"
      },
      "outputs": [],
      "source": [
        "# Download the training set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mLij6qde6Ox"
      },
      "outputs": [],
      "source": [
        "# Download the validation set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy"
      },
      "source": [
        "Then unzip both archives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip training set\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "# Unzip validation set\n",
        "local_zip = './validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./validation-horse-or-human')\n",
        "\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "Similar to the previous lab, you will define the directories containing your images. This time, you will include those with validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR_M9nWN-K8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBYtA_Zd8_T"
      },
      "source": [
        "Now see what the filenames look like in these directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PIP1rkmeAYS"
      },
      "outputs": [],
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n",
        "\n",
        "validation_horse_names = os.listdir(validation_horse_dir)\n",
        "print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n",
        "\n",
        "validation_human_names = os.listdir(validation_human_dir)\n",
        "print(f'VAL SET HUMANS: {validation_human_names[:10]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqN5KbafhLI"
      },
      "source": [
        "You can find out the total number of horse and human images in the directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XHh2xSfgie"
      },
      "outputs": [],
      "source": [
        "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
        "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
        "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
        "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Q0-_5UAv-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvHzGCxXkqp"
      },
      "source": [
        "Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr8GxjOU8in"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "You will define the same model architecture as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "You can review the network architecture and the output shapes with `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKj8392nbgP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "You will also use the same compile settings as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Now you will setup the data generators. It will mostly be the same as last time but notice the additional code to also prepare the validation data. It will need to be instantiated separately and also scaled to have `[0,1]` range of pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClebU9NJg99G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        # Since you use binary_crossentropy loss, you need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 128 using validation_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        './validation-horse-or-human/',  # This is the source directory for validation images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=32,\n",
        "        # Since you use binary_crossentropy loss, you need binary labels\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "Now train the model for 15 epochs. Here, you will pass parameters for `validation_data` and `validation_steps`. With these, you will notice additional outputs in the print statements: `val_loss` and `val_accuracy`. Notice that as you train with more epochs, your training accuracy might go up but your validation accuracy goes down. This can be a sign of overfitting and you need to prevent your model from reaching this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "### Model Prediction\n",
        "\n",
        "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n",
        "\n",
        "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it.\n",
        "\n",
        "_For Safari users: please comment out or skip the code block below, uncomment the next code block and run it._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoWp43WxJDNT"
      },
      "outputs": [],
      "source": [
        "## CODE BLOCK FOR NON-SAFARI BROWSERS\n",
        "## SAFARI USERS: PLEASE SKIP THIS BLOCK AND RUN THE NEXT ONE INSTEAD\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size=(300, 300))\n",
        "  x = img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJV8rdWU0NlM"
      },
      "source": [
        "`Safari` users will need to upload the images(s) manually in their workspace. Please follow the instructions, uncomment the code block below and run it.\n",
        "\n",
        "Instructions on how to upload image(s) manually in a Colab:\n",
        "\n",
        "1. Select the `folder` icon on the left `menu bar`.\n",
        "2. Click on the `folder with an arrow pointing upwards` named `..`\n",
        "3. Click on the `folder` named `tmp`.\n",
        "4. Inside of the `tmp` folder, `create a new folder` called `images`. You'll see the `New folder` option by clicking the `3 vertical dots` menu button next to the `tmp` folder.\n",
        "5. Inside of the new `images` folder, upload an image(s) of your choice, preferably of either a horse or a human. Drag and drop the images(s) on top of the `images` folder.\n",
        "6. Uncomment and run the code block below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyIcglKE0MpY"
      },
      "outputs": [],
      "source": [
        "# # CODE BLOCK FOR SAFARI USERS\n",
        "\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.utils import load_img, img_to_array\n",
        "# import os\n",
        "\n",
        "# images = os.listdir(\"/tmp/images\")\n",
        "\n",
        "# print(images)\n",
        "\n",
        "# for i in images:\n",
        "#  print()\n",
        "#  # predicting images\n",
        "#  path = '/tmp/images/' + i\n",
        "#  img = load_img(path, target_size=(300, 300))\n",
        "#  x = img_to_array(img)\n",
        "#  x /= 255\n",
        "#  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "#  images = np.vstack([x])\n",
        "#  classes = model.predict(images, batch_size=10)\n",
        "#  print(classes[0])\n",
        "#  if classes[0]>0.5:\n",
        "#    print(i + \" is a human\")\n",
        "#  else:\n",
        "#    print(i + \" is a horse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "As before, you can plot how the features are transformed as it goes through each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5tES8rXFjux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.utils import img_to_array, load_img\n",
        "\n",
        "# Define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so you can have them as part of the plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    \n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "    \n",
        "      # Tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    \n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "outputs": [],
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C1_W4_Lab_2_image_generator_with_validation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}